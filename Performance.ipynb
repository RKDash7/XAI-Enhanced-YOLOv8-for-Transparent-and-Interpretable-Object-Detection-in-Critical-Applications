{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ4GTSxuyPhCsreYHIEoO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RKDash7/XAI-Enhanced-YOLOv8-for-Transparent-and-Interpretable-Object-Detection-in-Critical-Applications/blob/main/Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIP30DmeUnhs"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "model = YOLO('yolov8n.pt')  # load model YOLO or SSD or Faster R-CNN\n",
        "results = model('your_image.jpg')  # run inference\n",
        "'''\n",
        "# If processing a single image, results is a Results object\n",
        "# For multiple images, results will be a list of Results objects\n",
        "\n",
        "# Access detections for the first (or only) image\n",
        "detections1 = results[0].boxes  # 'boxes' contains the detection data\n",
        "\n",
        "# Get detection data: tensor with shape [num_detections, 6] (xmin, ymin, xmax, ymax, confidence, class)\n",
        "detections_tensor = detections1.data  # or detections.xyxy\n",
        "\n",
        "# Total detections\n",
        "total_detections = len(detections_tensor)\n",
        "\n",
        "# Set confidence threshold\n",
        "conf_threshold = 0.75\n",
        "\n",
        "# Filter detections based on confidence\n",
        "filtered_detections = detections_tensor[detections_tensor[:, 4] >= conf_threshold]\n",
        "\n",
        "# Calculate drop percentage\n",
        "if total_detections > 0:\n",
        "    drop_percentage = (1 - len(filtered_detections) / total_detections) * 100\n",
        "else:\n",
        "    drop_percentage = 0\n",
        "\n",
        "print(f'Drop percentage: {drop_percentage:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have:\n",
        "# - results: model inference results for an image\n",
        "# - ground_truth_box: [xmin, ymin, xmax, ymax]\n",
        "\n",
        "# Get detections\n",
        "detections = results[0].boxes\n",
        "detections_data = detections.data\n",
        "\n",
        "# Find top detection by confidence\n",
        "# Move tensor to CPU before converting to numpy\n",
        "top_detection = detections_data[np.argmax(detections_data[:, 4].cpu().numpy())]\n",
        "\n",
        "# Move top_detection to CPU before converting to numpy if needed\n",
        "xmin, ymin, xmax, ymax = top_detection[:4].cpu().numpy()\n",
        "confidence = top_detection[4].item()\n",
        "\n",
        "# Compute center point\n",
        "center_x = (xmin + xmax) / 2\n",
        "center_y = (ymin + ymax) / 2\n",
        "\n",
        "# Check if center point is inside ground truth box\n",
        "# This check requires ground_truth_box to be defined elsewhere in the notebook\n",
        "# if (xmin <= center_x <= xmax) and (ymin <= center_y <= ymax):\n",
        "#     print(\"Hit\")\n",
        "# else:\n",
        "#     print(\"Miss\")\n",
        "\n",
        "# Note: The original code attempts to check if the center of the *predicted* bounding box is inside the *predicted* bounding box itself, which will always be true.\n",
        "# It seems like the intention might have been to compare the predicted box to a ground truth box.\n",
        "# If you have ground truth bounding box information, replace the commented-out check with a comparison to the ground truth box.\n",
        "\n",
        "print(f\"Top detection confidence: {confidence:.2f}\")\n",
        "print(f\"Top detection bounding box (xmin, ymin, xmax, ymax): {xmin:.2f}, {ymin:.2f}, {xmax:.2f}, {ymax:.2f}\")\n",
        "print(f\"Top detection center (x, y): {center_x:.2f}, {center_y:.2f}\")"
      ],
      "metadata": {
        "id": "ZYqkk1zwUxCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_ground_truth(image_file, labels_dir='/content/lung--1/valid/labels/', img_width=640, img_height=480):\n",
        "    \"\"\"\n",
        "    Load ground truth boxes and classes for the given image.\n",
        "\n",
        "    Args:\n",
        "        image_file (Path): Path to the image file.\n",
        "        labels_dir (str): Directory containing label files.\n",
        "        img_width (int): Width of the image.\n",
        "        img_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "        gt_boxes (list of [xmin, ymin, xmax, ymax]): Bounding boxes in pixel coordinates.\n",
        "        gt_classes (list of int): Class labels.\n",
        "    \"\"\"\n",
        "    label_file = Path(labels_dir) / (image_file.stem + '.txt')\n",
        "    gt_boxes = []\n",
        "    gt_classes = []\n",
        "\n",
        "    if not label_file.exists():\n",
        "        return gt_boxes, gt_classes  # No ground truth for this image\n",
        "\n",
        "    with open(label_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            class_id = int(parts[0])\n",
        "            x_center, y_center, width, height = map(float, parts[1:])\n",
        "\n",
        "            # Convert normalized coordinates to pixel values\n",
        "            xmin = int((x_center - width / 2) * img_width)\n",
        "            ymin = int((y_center - height / 2) * img_height)\n",
        "            xmax = int((x_center + width / 2) * img_width)\n",
        "            ymax = int((y_center + height / 2) * img_height)\n",
        "\n",
        "            gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "            gt_classes.append(class_id)\n",
        "    return gt_boxes, gt_classes"
      ],
      "metadata": {
        "id": "9Xf6Y96VVBW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Compute the Intersection over Union (IoU) of two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1 (list or array): [xmin, ymin, xmax, ymax]\n",
        "        box2 (list or array): [xmin, ymin, xmax, ymax]\n",
        "\n",
        "    Returns:\n",
        "        float: IoU value between 0 and 1.\n",
        "    \"\"\"\n",
        "    xmin1, ymin1, xmax1, ymax1 = box1\n",
        "    xmin2, ymin2, xmax2, ymax2 = box2\n",
        "\n",
        "    # Calculate intersection coordinates\n",
        "    inter_xmin = max(xmin1, xmin2)\n",
        "    inter_ymin = max(ymin1, ymin2)\n",
        "    inter_xmax = min(xmax1, xmax2)\n",
        "    inter_ymax = min(ymax1, ymax2)\n",
        "\n",
        "    # Compute area of intersection\n",
        "    inter_width = max(0, inter_xmax - inter_xmin)\n",
        "    inter_height = max(0, inter_ymax - inter_ymin)\n",
        "    inter_area = inter_width * inter_height\n",
        "\n",
        "    # Compute areas of each box\n",
        "    area1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
        "    area2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
        "\n",
        "    # Compute union area\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if union_area == 0:\n",
        "        return 0\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = inter_area / union_area\n",
        "    return iou"
      ],
      "metadata": {
        "id": "99q__QyIVFxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to dataset images and labels\n",
        "images_path = Path('/content/lung--1/valid/images/')\n",
        "labels_path = Path('/content/lung--1/valid/labels/')  # if available\n",
        "\n",
        "# Initialize counters\n",
        "total_images = 0\n",
        "correct_detections = 0\n",
        "\n",
        "# IoU threshold for considering a detection correct\n",
        "iou_threshold = 0.05\n",
        "\n",
        "# Loop through images\n",
        "for image_file in images_path.glob('*.jpg'):\n",
        "    total_images += 1\n",
        "    # Run inference\n",
        "    results1 = model(str(image_file))\n",
        "    preds = results1[0]  # predictions: (xmin, ymin, xmax, ymax, confidence, class)\n",
        "\n",
        "    # Load ground truth boxes and classes for this image\n",
        "    # You need to implement loading your labels here\n",
        "    gt_boxes, gt_classes = load_ground_truth(image_file)\n",
        "\n",
        "    # For simplicity, assume 1 primary object per image\n",
        "    if len(gt_boxes) == 0:\n",
        "        print(f\"No ground truth for {image_file}\")\n",
        "        continue  # skip if no ground truth\n",
        "\n",
        "    # Check if any predicted box matches ground truth\n",
        "    detection_matched = False\n",
        "    # Access detection data from the 'boxes' attribute\n",
        "    if preds.boxes: # Check if any boxes were detected\n",
        "      for pred in preds.boxes.data:\n",
        "          pred_box = pred[:4].cpu().numpy()\n",
        "          pred_cls = int(pred[5].cpu().item())\n",
        "\n",
        "          for gt_box, gt_cls in zip(gt_boxes, gt_classes):\n",
        "              iou = compute_iou(pred_box, gt_box)\n",
        "              if iou >= iou_threshold and pred_cls == gt_cls:\n",
        "                  detection_matched = True\n",
        "                  break\n",
        "          if detection_matched:\n",
        "              break\n",
        "\n",
        "    if detection_matched:\n",
        "        correct_detections += 1\n",
        "\n",
        "accuracy = correct_detections / total_images if total_images > 0 else 0\n",
        "print(f\"Detection Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "D9kvNrKlVJTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' is your trained YOLOv8 model and 'fig1' contains test image paths\n",
        "# Initialize the YOLOv8LRP explainer\n",
        "lrp = YOLOv8LRP(model, power=2, eps=1e-05, device='cpu') # Use CPU as per previous attempts\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = fig1[3] # Use one of the test images\n",
        "image = Image.open(image_path)\n",
        "desired_size = (512, 640)\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(desired_size),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "image_tensor = transform(image).to('cpu').float()\n",
        "\n",
        "# Compute the explanation\n",
        "explanation_lrp = lrp.explain(image_tensor, cls='Consolidation', contrastive=False).cpu() # Assuming 'Consolidation' is a class in your model\n",
        "\n",
        "# Compute metrics\n",
        "avg_drop = AverageDrop(lrp)\n",
        "increase_confidence = IncreaseConfidence(lrp)\n",
        "pointing_game = PointingGame(lrp)\n",
        "\n",
        "# Calculate and print the metrics\n",
        "avg_drop_score = avg_drop(image_tensor, explanation_lrp, cls='Consolidation')\n",
        "increase_confidence_score = increase_confidence(image_tensor, explanation_lrp, cls='Consolidation')\n",
        "pointing_game_score = pointing_game(image_tensor, explanation_lrp, cls='Consolidation')\n",
        "\n",
        "\n",
        "print(f\"Average Drop % for 'Consolidation': {avg_drop_score}\")\n",
        "print(f\"Increase in Confidence for 'Consolidation': {increase_confidence_score}\")\n",
        "print(f\"Pointing Game for 'Consolidation': {pointing_game_score}\")"
      ],
      "metadata": {
        "id": "wyzyRcZ9VdUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}