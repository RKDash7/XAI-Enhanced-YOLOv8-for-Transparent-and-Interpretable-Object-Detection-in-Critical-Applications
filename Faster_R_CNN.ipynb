{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwfDH7i+H8WY4kZCzaG940",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RKDash7/XAI-Enhanced-YOLOv8-for-Transparent-and-Interpretable-Object-Detection-in-Critical-Applications/blob/main/Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMCfKezfQCo4"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# ----------------------\n",
        "# 1. Custom Dataset Class for YOLO TXT\n",
        "# ----------------------\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transforms=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transforms = transforms\n",
        "        self.images = sorted(os.listdir(img_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.images[idx].replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        # Read YOLO label file\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "\n",
        "                # Convert normalized coords to absolute (xmin, ymin, xmax, ymax)\n",
        "                xmin = (x_center - width / 2) * w\n",
        "                ymin = (y_center - height / 2) * h\n",
        "                xmax = (x_center + width / 2) * w\n",
        "                ymax = (y_center + height / 2) * h\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(int(class_id) + 1)  # +1 because 0 is background in Faster R-CNN\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((len(labels),), dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "# ----------------------\n",
        "# 2. Transform Function\n",
        "# ----------------------\n",
        "def get_transform():\n",
        "    return torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "# ----------------------\n",
        "# 3. Load Train & Validation Data\n",
        "# ----------------------\n",
        "train_dataset = YOLODataset(\"/content/Drowsiness-Detectin-Using-Yolov8-1/train/images/\", \"/content/Drowsiness-Detectin-Using-Yolov8-1/train/labels/\", transforms=get_transform())\n",
        "val_dataset   = YOLODataset(\"/content/Drowsiness-Detectin-Using-Yolov8-1/valid/images/\", \"/content/Drowsiness-Detectin-Using-Yolov8-1/valid/labels/\", transforms=get_transform())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# ----------------------\n",
        "# 4. Model Setup\n",
        "# ----------------------\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = get_model(num_classes=4)  # Example: 1 class + background\n",
        "model.to(device)\n",
        "\n",
        "# ----------------------\n",
        "# 5. Optimizer\n",
        "# ----------------------\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# ----------------------\n",
        "# 6. Training Loop\n",
        "# ----------------------\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ----------------------\n",
        "# 7. Validation (Inference)\n",
        "# ----------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "        print(outputs)  # Contains 'boxes', 'labels', 'scores'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "\n",
        "# Redefine fig1 with paths that are guaranteed to exist\n",
        "# Let's use the dummy images created during the data preparation step\n",
        "fig1 = [\n",
        "    \"/content/Drowsiness-Detectin-Using-Yolov8-1/test/images/2623_jpg.rf.47b6274cf4abfc681aa03d278227689a.jpg\",\n",
        "    \"/content/Drowsiness-Detectin-Using-Yolov8-1/test/images/P1042797_720_mp4-202_jpg.rf.ad48eb832c35734f7c8427aba3a353d8.jpg\"\n",
        "]\n",
        "\n",
        "# Define the box_label function\n",
        "def box_label(image, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
        "  lw = max(round(sum(image.shape) / 2 * 0.003), 2)\n",
        "  p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "  cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
        "  if label:\n",
        "    tf = max(lw - 1, 1)  # font thickness\n",
        "    w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]  # text width, height\n",
        "    outside = p1[1] - h >= 3\n",
        "    p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "    cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "    cv2.putText(image,\n",
        "                label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
        "                0,\n",
        "                lw / 3,\n",
        "                txt_color,\n",
        "                thickness=tf,\n",
        "                lineType=cv2.LINE_AA)\n",
        "\n",
        "# Define the plot_bboxes function\n",
        "def plot_bboxes(image, boxes, labels=[], colors=[], score=True, conf=None):\n",
        "  #Define COCO Labels\n",
        "  if labels == []:\n",
        "    labels = {0:u'Background', 1: u'Alert', 2: u'Microsleep', 3: u'Yawn'}\n",
        "  #Define colors\n",
        "  if colors == []:\n",
        "    colors = [(89, 161, 197),(67, 161, 255),(19, 222, 24),(186, 55, 2)]\n",
        "\n",
        "  #plot each boxes\n",
        "  for box in boxes:\n",
        "    #add score in label if score=True\n",
        "    if score :\n",
        "      # Ensure the class index is within the bounds of the labels dictionary\n",
        "      class_index = int(box[-1])\n",
        "      if class_index in labels:\n",
        "          label = labels[class_index] + \" \" + str(round(100 * float(box[-2]),1)) + \"%\"\n",
        "      else:\n",
        "          label = f\"Unknown Class {class_index} \" + str(round(100 * float(box[-2]),1)) + \"%\"\n",
        "    else :\n",
        "      class_index = int(box[-1])\n",
        "      if class_index in labels:\n",
        "          label = labels[class_index]\n",
        "      else:\n",
        "          label = f\"Unknown Class {class_index}\"\n",
        "\n",
        "    #filter every box under conf threshold if conf threshold setted\n",
        "    if conf is not None:\n",
        "      if box[-2] > conf:\n",
        "        color = colors[class_index % len(colors)] # Use modulo for color indexing\n",
        "        box_label(image, box, label, color)\n",
        "    else:\n",
        "      color = colors[class_index % len(colors)] # Use modulo for color indexing\n",
        "      box_label(image, box, label, color)\n",
        "\n",
        "  #show image\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "  except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "  if IN_COLAB:\n",
        "    cv2_imshow(image) #if used in Colab\n",
        "  else :\n",
        "    cv2.imshow(\"Inference Result\", image) #if used in Python\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# 1. Load the fine-tuned model weights onto the model architecture.\n",
        "# Ensure the model architecture is defined (it was defined in the previous subtasks)\n",
        "# Assuming 'model' object from previous steps is available.\n",
        "# If not, you would need to re-define it here with the correct number of classes.\n",
        "# For this example, we assume the model architecture is still in memory.\n",
        "\n",
        "# Load the state dictionary\n",
        "# Use the path where you saved your best model weights\n",
        "# If 'fasterrcnn_finetuned_epoch_10.pth' is not the correct path, update it.\n",
        "#model.load_state_dict(torch.load(\"fasterrcnn_finetuned_epoch_10.pth\"))\n",
        "\n",
        "\n",
        "# 2. Set the model to evaluation mode\n",
        "#model.eval()\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Select an image path from the reloaded fig1 list.\n",
        "image_path = fig1[0] # Using the first dummy image\n",
        "\n",
        "# 4. Load and preprocess the selected image.\n",
        "img_pil = Image.open(image_path).convert(\"RGB\")\n",
        "transform = T.Compose([T.ToTensor()]) # Simple ToTensor transform\n",
        "input_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "# 5. Perform inference by passing the preprocessed image tensor through the model.\n",
        "# Make sure to wrap the inference in a torch.no_grad() block.\n",
        "with torch.no_grad():\n",
        "    prediction = model(input_tensor)\n",
        "\n",
        "# 6. Process the raw output of the model to extract bounding boxes, labels, and confidence scores.\n",
        "# The output 'prediction' is a list of dictionaries, one for each image in the batch.\n",
        "# For a single image, we take the first dictionary: prediction[0]\n",
        "# It contains 'boxes', 'labels', and 'scores'.\n",
        "boxes = prediction[0]['boxes']\n",
        "labels = prediction[0]['labels']\n",
        "scores = prediction[0]['scores']\n",
        "\n",
        "# Apply a confidence threshold and NMS (optional but recommended for cleaner results)\n",
        "conf_threshold = 0.5 # You can adjust this threshold\n",
        "\n",
        "# Filter based on confidence\n",
        "keep = scores > conf_threshold\n",
        "boxes = boxes[keep]\n",
        "labels = labels[keep]\n",
        "scores = scores[keep]\n",
        "\n",
        "\n",
        "# Convert bounding boxes and image to numpy for visualization\n",
        "img_np = np.array(img_pil)\n",
        "boxes_np = boxes.cpu().numpy()\n",
        "labels_np = labels.cpu().numpy()\n",
        "scores_np = scores.cpu().numpy()\n",
        "\n",
        "\n",
        "# 7. Visualize the results by drawing the detected bounding boxes and labels on the original image.\n",
        "# Use the previously defined plot_bboxes function.\n",
        "# The plot_bboxes function expects boxes in [xmin, ymin, xmax, ymax] format, which boxes_np is.\n",
        "# It also expects labels and scores.\n",
        "# You might need to map the integer labels back to class names if you want to display names.\n",
        "# Assuming a simple mapping for demonstration (adjust based on your dataset's classes)\n",
        "class_names = {1: 'Alert', 2: 'Microsleep',3:'Yawn'} # Adjust based on your 10 classes (including background). The dummy data only has class1 and class2.\n",
        "\n",
        "# Create a list of dictionaries in the format expected by plot_bboxes\n",
        "# plot_bboxes seems to expect a different format based on the provided code.\n",
        "# Let's re-examine plot_bboxes signature: def plot_bboxes(image, boxes, labels=[], colors=[], score=True, conf=None):\n",
        "# It iterates through 'boxes' and expects each element to be a tensor/array with [xmin, ymin, xmax, ymax, confidence, class_id].\n",
        "# Let's format our detections accordingly.\n",
        "detections_for_plotting = []\n",
        "for i in range(len(boxes_np)):\n",
        "    # Combine box, score, and label into one array\n",
        "    detection_info = np.concatenate((boxes_np[i], [scores_np[i]], [labels_np[i]]))\n",
        "    detections_for_plotting.append(detection_info)\n",
        "\n",
        "# Call plot_bboxes\n",
        "# Ensure cv2_imshow is available if running in Colab.\n",
        "# If not in Colab, you might need to use matplotlib or cv2.imshow directly.\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    # If not in Colab, define a simple show function or use matplotlib\n",
        "    def cv2_imshow(img):\n",
        "        cv2.imshow(\"Inference Result\", img)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# Convert the image back to BGR if necessary for cv2 functions\n",
        "img_bgr_for_plotting = cv2.cvtColor(img_np.copy(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Call plot_bboxes with the formatted detections\n",
        "plot_bboxes(img_bgr_for_plotting, detections_for_plotting, labels=class_names, score=True, conf=conf_threshold)\n",
        "\n",
        "# 8. Display the image with the inference results.\n",
        "# plot_bboxes already handles displaying the image using cv2_imshow or cv2.imshow\n",
        "print(\"Inference complete. Displaying image with detections:\")"
      ],
      "metadata": {
        "id": "s_NC-5tuQKjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}