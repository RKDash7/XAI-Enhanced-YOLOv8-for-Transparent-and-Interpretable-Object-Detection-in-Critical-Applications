{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR+5mjNrs7wkqlqSpXwimZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RKDash7/XAI-Enhanced-YOLOv8-for-Transparent-and-Interpretable-Object-Detection-in-Critical-Applications/blob/main/XAI_YOLOv8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam import (EigenCAM, EigenGradCAM, GradCAM, GradCAMPlusPlus,\n",
        "                              HiResCAM, LayerCAM, RandomCAM, XGradCAM)\n",
        "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
        "from pytorch_grad_cam.utils.image import scale_cam_image, show_cam_on_image\n",
        "from ultralytics.nn.tasks import attempt_load_weights\n",
        "from ultralytics.utils.ops import non_max_suppression, xywh2xyxy"
      ],
      "metadata": {
        "id": "gMeXLezWRZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model=YOLO(\"yolov8s.yaml\")\n",
        "model=YOLO(\"yolov8s.pt\")\n",
        "model.info()"
      ],
      "metadata": {
        "id": "ObFQ9GgWRJiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Q2QrYKQnuy"
      },
      "outputs": [],
      "source": [
        "\n",
        "rf = Roboflow(api_key=\"zHOMcsaq56QWd6sO4BWy\")\n",
        "project = rf.workspace(\"utopialab\").project(\"rice-leaf-wfax3\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results=model.train(data=\"/content/Rice-Leaf-2/data.yaml\",epochs=25,imgsz=640)"
      ],
      "metadata": {
        "id": "Isr6o6LnQ2Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rf = Roboflow(api_key=\"zHOMcsaq56QWd6sO4BWy\")\n",
        "project = rf.workspace(\"drowsiness-detecion-yolov8\").project(\"drowsiness-detectin-using-yolov8\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "H8Bp6NvPQxNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=model.train(data=\"/content/Drowsiness-Detectin-Using-Yolov8-1/data.yaml\",epochs=25,imgsz=640)"
      ],
      "metadata": {
        "id": "cptRkEjsRAma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rf = Roboflow(api_key=\"zHOMcsaq56QWd6sO4BWy\")\n",
        "project = rf.workspace(\"shraddha-patil-6unld\").project(\"lung-lzjkc\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "RfuntXM_Q1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=model.train(data=\"/content/lung--1/data.yaml\",epochs=1,imgsz=640,batch=16)"
      ],
      "metadata": {
        "id": "CfahQe_BRFgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img=cv2.imread('/content/Rice-Leaf-2/valid/images/DSC_0515_jpg.rf.72f76a20d6ed7ce7a732f6aa7211a4ab.jpg')\n",
        "image = np.asarray(img)"
      ],
      "metadata": {
        "id": "mro24UzvRRtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(image)\n",
        "results[0].plot()"
      ],
      "metadata": {
        "id": "D-KiyfhPRnGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ActivationsAndGradients:\n",
        "    \"\"\" Class for extracting activations and\n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model: torch.nn.Module,\n",
        "                 target_layers: List[torch.nn.Module],\n",
        "                 reshape_transform: Optional[callable]) -> None:  # type: ignore\n",
        "        \"\"\"\n",
        "        Initializes the ActivationsAndGradients object.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The neural network model.\n",
        "            target_layers (List[torch.nn.Module]): List of target layers from which to extract activations and gradients.\n",
        "            reshape_transform (Optional[callable]): A function to transform the shape of the activations and gradients if needed.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.handles = []\n",
        "        for target_layer in target_layers:\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_activation))\n",
        "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
        "            # we don't use backward hook to record gradients.\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_gradient))\n",
        "\n",
        "    def save_activation(self, module: torch.nn.Module,\n",
        "                        input: Union[torch.Tensor, Tuple[torch.Tensor, ...]],\n",
        "                        output: torch.Tensor) -> None:\n",
        "        \"\"\"\n",
        "        Saves the activation of the targeted layer.\n",
        "\n",
        "        Args:\n",
        "            module (torch.nn.Module): The targeted layer module.\n",
        "            input (Union[torch.Tensor, Tuple[torch.Tensor, ...]]): The input to the targeted layer.\n",
        "            output (torch.Tensor): The output activation of the targeted layer.\n",
        "        \"\"\"\n",
        "        activation = output\n",
        "\n",
        "        if self.reshape_transform is not None:\n",
        "            activation = self.reshape_transform(activation)\n",
        "        self.activations.append(activation.cpu().detach())\n",
        "\n",
        "    def save_gradient(self, module: torch.nn.Module,\n",
        "                      input: Union[torch.Tensor, Tuple[torch.Tensor, ...]],\n",
        "                      output: torch.Tensor) -> None:\n",
        "        \"\"\"\n",
        "        Saves the gradient of the targeted layer.\n",
        "\n",
        "        Args:\n",
        "            module (torch.nn.Module): The targeted layer module.\n",
        "            input (Union[torch.Tensor, Tuple[torch.Tensor, ...]]): The input to the targeted layer.\n",
        "            output (torch.Tensor): The output activation of the targeted layer.\n",
        "        \"\"\"\n",
        "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
        "            # You can only register hooks on tensor requires grad.\n",
        "            return\n",
        "\n",
        "        # Gradients are computed in reverse order\n",
        "        def _store_grad(grad: torch.Tensor) -> None:\n",
        "            if self.reshape_transform is not None:\n",
        "                grad = self.reshape_transform(grad)\n",
        "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
        "\n",
        "        output.register_hook(_store_grad)\n",
        "\n",
        "    def post_process(self, result: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Post-processes the result.\n",
        "\n",
        "        Args:\n",
        "            result (torch.Tensor): The result tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, np.ndarray]: A tuple containing the post-processed result.\n",
        "        \"\"\"\n",
        "        logits_ = result[:, 4:]\n",
        "        boxes_ = result[:, :4]\n",
        "        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n",
        "        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[\n",
        "            indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n",
        "\n",
        "    def __call__(self, x: torch.Tensor) -> List[List[Union[torch.Tensor, np.ndarray]]]:\n",
        "        \"\"\"\n",
        "        Calls the ActivationsAndGradients object.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            List[List[Union[torch.Tensor, np.ndarray]]]: A list containing activations and gradients.\n",
        "        \"\"\"\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        model_output = self.model(x)\n",
        "        post_result, pre_post_boxes, post_boxes = self.post_process(\n",
        "            model_output[0])\n",
        "        return [[post_result, pre_post_boxes]]\n",
        "\n",
        "    def release(self) -> None:\n",
        "        \"\"\"Removes hooks.\"\"\"\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "\n",
        "\n",
        "class yolov8_target(torch.nn.Module):\n",
        "    def __init__(self, ouput_type, conf, ratio) -> None:\n",
        "        super().__init__()\n",
        "        self.ouput_type = ouput_type\n",
        "        self.conf = conf\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, data):\n",
        "        post_result, pre_post_boxes = data\n",
        "        result = []\n",
        "        for i in range(post_result.size(0)):\n",
        "            if float(post_result[i].max()) >= self.conf:\n",
        "                if self.ouput_type == 'class' or self.ouput_type == 'all':\n",
        "                    result.append(post_result[i].max())\n",
        "                if self.ouput_type == 'box' or self.ouput_type == 'all':\n",
        "                    for j in range(4):\n",
        "                        result.append(pre_post_boxes[i, j])\n",
        "        return sum(result)\n",
        "\n",
        "\n",
        "class yolov8_heatmap:\n",
        "    \"\"\"\n",
        "    This class is used to implement the YOLOv8 target layer.\n",
        "\n",
        "     Args:\n",
        "            weight (str): The path to the checkpoint file.\n",
        "            device (str): The device to use for inference. Defaults to \"cuda:0\" if a GPU is available, otherwise \"cpu\".\n",
        "            method (str): The method to use for computing the CAM. Defaults to \"EigenGradCAM\".\n",
        "            layer (list): The indices of the layers to use for computing the CAM. Defaults to [10, 12, 14, 16, 18, -3].\n",
        "            conf_threshold (float): The confidence threshold for detections. Defaults to 0.2.\n",
        "            ratio (float): The ratio of maximum scores to return. Defaults to 0.02.\n",
        "            show_box (bool): Whether to show bounding boxes with the CAM. Defaults to True.\n",
        "            renormalize (bool): Whether to renormalize the CAM to be in the range [0, 1] across the entire image. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        A tensor containing the output.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            weight: str,\n",
        "            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "            method=\"EigenGradCAM\",\n",
        "            layer=[12, 17, 21],\n",
        "            conf_threshold=0.2,\n",
        "            ratio=0.02,\n",
        "            show_box=True,\n",
        "            renormalize=False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the YOLOv8 heatmap layer.\n",
        "        \"\"\"\n",
        "        device = device\n",
        "        backward_type = \"all\"\n",
        "        ckpt = torch.load(weight, weights_only=False) # Added weights_only=False\n",
        "        model_names = ckpt['model'].names\n",
        "        model = attempt_load_weights(weight, device)\n",
        "        model.info()\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad_(True)\n",
        "        model.eval()\n",
        "\n",
        "        target = yolov8_target(backward_type, conf_threshold, ratio)\n",
        "        target_layers = [model.model[l] for l in layer]\n",
        "\n",
        "        method = eval(method)(model, target_layers,\n",
        "                              use_cuda=device.type == 'cuda')\n",
        "        method.activations_and_grads = ActivationsAndGradients(\n",
        "            model, target_layers, None)\n",
        "\n",
        "        colors = np.random.uniform(\n",
        "            0, 255, size=(len(model_names), 3)).astype(int)\n",
        "        self.__dict__.update(locals())\n",
        "\n",
        "    def post_process(self, result):\n",
        "        \"\"\"\n",
        "        Perform non-maximum suppression on the detections and process results.\n",
        "\n",
        "        Args:\n",
        "            result (torch.Tensor): The raw detections from the model.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Filtered and processed detections.\n",
        "        \"\"\"\n",
        "        # Perform non-maximum suppression\n",
        "        processed_result = non_max_suppression(\n",
        "            result,\n",
        "            conf_thres=self.conf_threshold,  # Use the class's confidence threshold\n",
        "            iou_thres=0.45  # Intersection over Union threshold\n",
        "        )\n",
        "\n",
        "        # If no detections, return an empty tensor\n",
        "        if len(processed_result) == 0 or processed_result[0].numel() == 0:\n",
        "            return torch.empty(0, 6)  # Return an empty tensor with 6 columns\n",
        "\n",
        "        # Take the first batch of detections (assuming single image)\n",
        "        detections = processed_result[0]\n",
        "\n",
        "        # Filter detections based on confidence\n",
        "        mask = detections[:, 4] >= self.conf_threshold\n",
        "        filtered_detections = detections[mask]\n",
        "\n",
        "        return filtered_detections\n",
        "\n",
        "    def draw_detections(self, box, color, name, img):\n",
        "        \"\"\"\n",
        "        Draw bounding boxes and labels on an image for multiple detections.\n",
        "\n",
        "        Args:\n",
        "            box (torch.Tensor or np.ndarray): The bounding box coordinates in the format [x1, y1, x2, y2]\n",
        "            color (list): The color of the bounding box in the format [B, G, R]\n",
        "            name (str): The label for the bounding box.\n",
        "            img (np.ndarray): The image on which to draw the bounding box\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The image with the bounding box drawn.\n",
        "        \"\"\"\n",
        "        # Ensure box coordinates are integers\n",
        "        xmin, ymin, xmax, ymax = map(int, box[:4])\n",
        "\n",
        "        # Draw rectangle\n",
        "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax),\n",
        "                      tuple(int(x) for x in color), 2)\n",
        "        name=name+\" \" + str(0.90)\n",
        "        # Draw label\n",
        "        cv2.putText(img, name, (xmin, ymin - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8, tuple(int(x) for x in color), 2,\n",
        "                    lineType=cv2.LINE_AA)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def renormalize_cam_in_bounding_boxes(\n",
        "            self,\n",
        "            boxes: np.ndarray,  # type: ignore\n",
        "            image_float_np: np.ndarray,  # type: ignore\n",
        "            grayscale_cam: np.ndarray,  # type: ignore\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Normalize the CAM to be in the range [0, 1]\n",
        "        inside every bounding boxes, and zero outside of the bounding boxes.\n",
        "\n",
        "        Args:\n",
        "            boxes (np.ndarray): The bounding boxes.\n",
        "            image_float_np (np.ndarray): The image as a numpy array of floats in the range [0, 1].\n",
        "            grayscale_cam (np.ndarray): The CAM as a numpy array of floats in the range [0, 1].\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The renormalized CAM.\n",
        "        \"\"\"\n",
        "        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
        "        for x1, y1, x2, y2 in boxes:\n",
        "            x1, y1 = max(x1, 0), max(y1, 0)\n",
        "            x2, y2 = min(grayscale_cam.shape[1] - 1,\n",
        "                         x2), min(grayscale_cam.shape[0] - 1, y2)\n",
        "            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(\n",
        "                grayscale_cam[y1:y2, x1:x2].copy())\n",
        "        renormalized_cam = scale_cam_image(renormalized_cam)\n",
        "        eigencam_image_renormalized = show_cam_on_image(\n",
        "            image_float_np, renormalized_cam, use_rgb=True)\n",
        "        return eigencam_image_renormalized\n",
        "\n",
        "    def renormalize_cam(self, boxes, image_float_np, grayscale_cam):\n",
        "        \"\"\"Normalize the CAM to be in the range [0, 1]\n",
        "        across the entire image.\"\"\"\n",
        "        renormalized_cam = scale_cam_image(grayscale_cam)\n",
        "        eigencam_image_renormalized = show_cam_on_image(\n",
        "            image_float_np, renormalized_cam, use_rgb=True)\n",
        "        return eigencam_image_renormalized\n",
        "\n",
        "    def process(self, img_path):\n",
        "        \"\"\"Process the input image and generate CAM visualization.\"\"\"\n",
        "        img = cv2.imread(img_path)\n",
        "        img = letterbox(img)[0]\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = np.float32(img) / 255.0\n",
        "\n",
        "        tensor = (\n",
        "            torch.from_numpy(np.transpose(img, axes=[2, 0, 1]))\n",
        "            .unsqueeze(0)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            grayscale_cam = self.method(tensor, [self.target])\n",
        "        except AttributeError as e:\n",
        "            print(e)\n",
        "            return\n",
        "\n",
        "        grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "        pred1 = self.model(tensor)[0]\n",
        "        pred = non_max_suppression(\n",
        "            pred1,\n",
        "            conf_thres=self.conf_threshold,\n",
        "            iou_thres=0.45\n",
        "        )[0]\n",
        "\n",
        "        # Debugging print\n",
        "\n",
        "        if self.renormalize:\n",
        "            cam_image = self.renormalize_cam(\n",
        "                pred[:, :4].cpu().detach().numpy().astype(np.int32),\n",
        "                img,\n",
        "                grayscale_cam\n",
        "            )\n",
        "        else:\n",
        "            cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        if self.show_box and len(pred) > 0:\n",
        "            for detection in pred:\n",
        "                detection = detection.cpu().detach().numpy()\n",
        "\n",
        "                # Get class index and confidence\n",
        "                class_index = int(detection[5])\n",
        "                conf = detection[4]\n",
        "\n",
        "                # Draw detection\n",
        "                cam_image = self.draw_detections(\n",
        "                    detection[:4],  # Box coordinates\n",
        "                    self.colors[class_index],  # Color for this class\n",
        "                    f\"{self.model_names[class_index]}\",  # Label with confidence\n",
        "                    cam_image,\n",
        "                )\n",
        "\n",
        "        cam_image = Image.fromarray(cam_image)\n",
        "        return cam_image\n",
        "\n",
        "    def __call__(self, img_path):\n",
        "        \"\"\"Generate CAM visualizations for one or more images.\n",
        "\n",
        "        Args:\n",
        "            img_path (str): Path to the input image or directory containing images.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if os.path.isdir(img_path):\n",
        "            image_list = []\n",
        "            for img_path_ in os.listdir(img_path):\n",
        "                img_pil = self.process(f\"{img_path}/{img_path_}\")\n",
        "                image_list.append(img_pil)\n",
        "            return image_list\n",
        "        else:\n",
        "            return [self.process(img_path)]"
      ],
      "metadata": {
        "id": "IZVNqJncSJfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def letterbox(\n",
        "    im: np.ndarray,\n",
        "    new_shape=(640, 640),\n",
        "    color=(114, 114, 114),\n",
        "    auto=True,\n",
        "    scaleFill=False,\n",
        "    scaleup=True,\n",
        "    stride=32,\n",
        "):\n",
        "    \"\"\"\n",
        "    Resize and pad image while meeting stride-multiple constraints.\n",
        "\n",
        "    Args:\n",
        "        im (numpy.ndarray): Input image.\n",
        "        new_shape (tuple, optional): Desired output shape. Defaults to (640, 640).\n",
        "        color (tuple, optional): Color of the border. Defaults to (114, 114, 114).\n",
        "        auto (bool, optional): Whether to automatically determine padding. Defaults to True.\n",
        "        scaleFill (bool, optional): Whether to stretch the image to fill the new shape. Defaults to False.\n",
        "        scaleup (bool, optional): Whether to scale the image up if necessary. Defaults to True.\n",
        "        stride (int, optional): Stride of the sliding window. Defaults to 32.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Letterboxed image.\n",
        "        tuple: Ratio of the resized image.\n",
        "        tuple: Padding sizes.\n",
        "\n",
        "    \"\"\"\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "\n",
        "    return im, ratio, (dw, dh)\n",
        "\n",
        "def display_images(images):\n",
        "    \"\"\"\n",
        "    Display a list of PIL images in a grid.\n",
        "\n",
        "    Args:\n",
        "        images (list[PIL.Image]): A list of PIL images to display.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(15, 7))\n",
        "    if len(images) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, img in zip(axes, images):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eMIE9lPgSjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# from YOLOv8_Explainer import yolov8_heatmap, display_images # Commented out the import from the library\n",
        "\n",
        "# The yolov8_heatmap and display_images functions are now defined in cell 26be86f9\n",
        "\n",
        "\n",
        "model_lrp = yolov8_heatmap(\n",
        "    weight='/content/drive/MyDrive/best_lungs.pt',#'/content/runs/detect/train/weights/best.pt',#'/content/drive/MyDrive/best_lungs.pt',#\"/content/drive/MyDrive/best (4).pt\",\n",
        "        #conf_threshold=0.4,\n",
        "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        method= \"EigenGradCAM\",\n",
        "        layer=[10, 12, 14, 16, 18,-3, -2],\n",
        "        conf_threshold=0.2,\n",
        "        ratio=0.01,\n",
        "        show_box=False,\n",
        "        renormalize=False,\n",
        ")\n",
        "\n",
        "imagelist = model_lrp(\n",
        "    img_path='/content/lung--1/test/images/000000000132_jpg.rf.7da3d7d0dbe7b8943884dc31dca88b63.jpg'#fig[3]#'/content/Drowsiness-Detectin-Using-Yolov8-1/valid/images/WIN_20211018_21_12_44_Pro_jpg.rf.c3ef2f8695ac6070253fc3d5120d42e8.jpg'\n",
        "    #fig[3]#'/content/Drowsiness-Detectin-Using-Yolov8-1/valid/images/yawn-930e3cfe-3043-11ec-ad5b-302432df5bc8_jpg.rf.576106b54b5df27f8cdff964960ac.jpg'#fig[1]#'/content/Drowsiness-Detectin-Using-Yolov8-1/valid/images/closed-eye-8737adc6-3043-11ec-a72a-302432df5bc8_jpg.rf.b27fba558b359e867a38728a680b3ff7.jpg',\n",
        "    )\n",
        "pred=imagelist\n",
        "display_images(imagelist)\n",
        "display_images(imagelist)"
      ],
      "metadata": {
        "id": "P9bKS6aZSPyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}